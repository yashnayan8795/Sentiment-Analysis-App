{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 14:10:19,631 [INFO] Initializing models...\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "2025-03-31 14:10:20,658 [INFO] Models initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Web Scraping and Sentiment Analysis Tool ====\n",
      "1. Analyze a new article\n",
      "2. View previously scraped articles\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 14:10:28,192 [INFO] Processing URL: https://edition.cnn.com/2025/03/30/europe/europe-defense-wake-up-ukraine-russia-trump-intl/index.html\n",
      "2025-03-31 14:10:30,844 [INFO] Article already exists in MongoDB. Skipping insertion.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Scraped Article Analysis =====\n",
      "Heading: ‘PATHETIC’ Europe may finally be waking up from its military slumber\n",
      "URL: https://edition.cnn.com/2025/03/30/europe/europe-defense-wake-up-ukraine-russia-trump-intl/index.html\n",
      "Meta Description: It was a televised ambush that many in Europe hope will stop a war.\n",
      "Summary: a televised ambush that many in Europe hope will stop a war . it was a lightning strike to the transatlantic alliance, dispelling lingering illusions . \"it is as if\n",
      "Overall Sentiment: positive\n",
      "====================================\n",
      "\n",
      "\n",
      "==== Web Scraping and Sentiment Analysis Tool ====\n",
      "1. Analyze a new article\n",
      "2. View previously scraped articles\n",
      "3. Exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-31 14:10:55,425 [INFO] MongoDB connection closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import async_timeout\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from pymongo import MongoClient\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Apply nest_asyncio to make async functions work in Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler(), logging.FileHandler(\"scraper.log\")]\n",
    ")\n",
    "\n",
    "# MongoDB Setup\n",
    "MONGO_URI = \"mongodb+srv://user:pass123ynm@cluster0.16nk9hq.mongodb.net/sentiment-analysis\"\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client[\"sentiment-analysis\"]\n",
    "collection = db[\"scraped_articles\"]\n",
    "\n",
    "# Initialize the transformers pipelines\n",
    "logging.info(\"Initializing models...\")\n",
    "try:\n",
    "    summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "    transformer_classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    logging.info(\"Models initialized successfully\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error initializing models: {e}\")\n",
    "    raise\n",
    "\n",
    "# Custom headers to avoid blocking\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/110.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Define sentiment label mapping\n",
    "LABEL_MAP = {\"NEGATIVE\": \"negative\", \"POSITIVE\": \"positive\"}\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'\\d+', '', text)                     # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)                 # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()            # Remove extra spaces\n",
    "    text = \" \".join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])\n",
    "    return text\n",
    "\n",
    "# Async function to fetch a URL with retry logic\n",
    "async def fetch(session, url, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            async with async_timeout.timeout(10):\n",
    "                async with session.get(url, headers=HEADERS) as response:\n",
    "                    response.raise_for_status()\n",
    "                    return await response.text()\n",
    "        except (aiohttp.ClientError, async_timeout.TimeoutError) as e:\n",
    "            logging.warning(f\"Attempt {attempt+1} for {url} failed: {e}\")\n",
    "            await asyncio.sleep(2)\n",
    "    logging.error(f\"Failed to fetch {url} after {retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# Async function to scrape a URL\n",
    "async def scrape_url(session, url):\n",
    "    html = await fetch(session, url)\n",
    "    if not html:\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Extract heading\n",
    "    heading_tag = soup.find(\"h1\") or soup.find(\"title\")\n",
    "    heading_text = heading_tag.get_text(strip=True) if heading_tag else \"No Heading\"\n",
    "\n",
    "    # Extract body\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    body_text = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
    "    \n",
    "    # Extract meta description\n",
    "    meta_desc = \"\"\n",
    "    meta_tag = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "    if meta_tag and meta_tag.get(\"content\"):\n",
    "        meta_desc = meta_tag.get(\"content\").strip()\n",
    "\n",
    "    return [heading_text, body_text, meta_desc, url]\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text):\n",
    "    try:\n",
    "        max_input_length = 512\n",
    "        if len(text) > max_input_length:\n",
    "            text = text[:max_input_length]\n",
    "\n",
    "        summary = summarizer(text, max_length=50, min_length=30, do_sample=False)\n",
    "        return summary[0]['summary_text']\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Summarization failed: {e}\")\n",
    "        return text[:300] + \"...\"\n",
    "\n",
    "# Function to analyze sentiment\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        max_input_length = 512\n",
    "        if len(text) > max_input_length:\n",
    "            text = text[:max_input_length]\n",
    "\n",
    "        result = transformer_classifier(text)[0]\n",
    "        sentiment = LABEL_MAP.get(result['label'], result['label'])\n",
    "        return sentiment, result['score']\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Sentiment analysis failed: {e}\")\n",
    "        return \"Error\", None\n",
    "\n",
    "# Store results in MongoDB\n",
    "def store_results(data):\n",
    "    if collection.find_one({\"url\": data[\"url\"]}):\n",
    "        logging.info(\"Article already exists in MongoDB. Skipping insertion.\")\n",
    "        return\n",
    "    collection.insert_one(data)\n",
    "    logging.info(\"Data stored in MongoDB.\")\n",
    "\n",
    "# Function to display the scraped data\n",
    "def display_results(data):\n",
    "    print(\"\\n===== Scraped Article Analysis =====\")\n",
    "    print(f\"Heading: {data['heading']}\")\n",
    "    print(f\"URL: {data['url']}\")\n",
    "    print(f\"Meta Description: {data['meta_description']}\")\n",
    "    print(f\"Summary: {data['summary']}\")\n",
    "    print(f\"Overall Sentiment: {data['overall_sentiment']}\")\n",
    "    print(\"====================================\\n\")\n",
    "\n",
    "# Function to display previously scraped articles\n",
    "def show_scraped_data():\n",
    "    articles = collection.find({})\n",
    "    if collection.count_documents({}) == 0:\n",
    "        print(\"No articles found in the database.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n==== Scraped Articles ====\")\n",
    "    for i, article in enumerate(articles, start=1):\n",
    "        print(f\"\\n--- Article {i} ---\")\n",
    "        print(f\"Heading: {article['heading']}\")\n",
    "        print(f\"URL: {article['url']}\")\n",
    "        print(f\"Overall Sentiment: {article['overall_sentiment']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Main function to process a URL\n",
    "async def process_url(url):\n",
    "    logging.info(f\"Processing URL: {url}\")\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        result = await scrape_url(session, url)\n",
    "\n",
    "    if not result:\n",
    "        logging.error(f\"Failed to scrape URL: {url}\")\n",
    "        return\n",
    "\n",
    "    heading, body, meta_desc, url = result\n",
    "    summary = summarize_text(body)\n",
    "    overall_sentiment, _ = analyze_sentiment(body)\n",
    "\n",
    "    data = {\n",
    "        \"heading\": heading,\n",
    "        \"meta_description\": meta_desc,\n",
    "        \"url\": url,\n",
    "        \"summary\": summary,\n",
    "        \"overall_sentiment\": overall_sentiment\n",
    "    }\n",
    "\n",
    "    # Store and display results\n",
    "    store_results(data)\n",
    "    display_results(data)\n",
    "    return data\n",
    "\n",
    "# Interactive menu\n",
    "async def interactive_menu():\n",
    "    while True:\n",
    "        print(\"\\n==== Web Scraping and Sentiment Analysis Tool ====\")\n",
    "        print(\"1. Analyze a new article\")\n",
    "        print(\"2. View previously scraped articles\")\n",
    "        print(\"3. Exit\")\n",
    "        choice = input(\"Enter your choice (1-3): \").strip()\n",
    "\n",
    "        if choice == \"1\":\n",
    "            url = input(\"Enter the URL of the article to analyze: \").strip()\n",
    "            if url:\n",
    "                await process_url(url)\n",
    "            else:\n",
    "                print(\"No URL provided.\")\n",
    "        elif choice == \"2\":\n",
    "            show_scraped_data()\n",
    "        elif choice == \"3\":\n",
    "            print(\"Exiting. Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        asyncio.run(interactive_menu())\n",
    "    except RuntimeError:\n",
    "        asyncio.get_event_loop().run_until_complete(interactive_menu())\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Unexpected error: {e}\")\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "    finally:\n",
    "        client.close()\n",
    "        logging.info(\"MongoDB connection closed.\")\n",
    "        client.close()\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
